<!-- date: 2019-12-22 -->
# What I read this week (December 22 - 28)

## [This is how AI bias really happens--and why it's so hard to fix](https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/)
**Author**: Karen Hao

**How I found this**: from the MIT technology review

**Summary**: it's easy to say that bias in AI comes from bias in training data, but that's an oversimplification; why does the training data contain biased? Why can't we just find unbiased training data, if that's the case? The reality is, of course, more complex, and Hao outlines three places where bias creeps in:

 * *framing the problem*, where you define what your goal is, how you compute it, and how it relates to business objectives.
 * *collecting the data*, where the data you collect is unrepresentative of reality (images of only white people), or it reflects existing prejudices (training on historical decisions about who to hire).
 * *preparing the data*, where you choose features that may lead to a more biased model.

Why, then, is it so hard to fix? Some reasons include:

 * *unknown unknowns*, where you don't know when a choice you make introduces bias, and it isn't apparent until much later
 * we have *imperfect processes* that aren't designed with the goal of reducing bias, like how you evaluate a model on data that's like the training dataset instead of in the real world.
 * *lack of social context*, like how "fair" means different things in different communities, how generalization in a CS setting is different than generalization in a social setting, etc.
 * *unclear definitions of fairness*, which doesn't have a clear definition in the real world, leave alone one that we can apply to statistical models.

**Thoughts**: this is good. I'm definitely guilty of using the term "AI bias" or "biased models" imprecisely, and pieces like this help me to think more about what that actually means and where those biases come from.

## [NIST study evaluates effects of age, race, sex on face recognition software](https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software)
**Author**: NIST

**How I found this**: from the newsletter [The Algorithm](https://www.the-algorithm.net/newsletter)

**Summary**: NIST studied 189 algorithms among 99 developers, and "found empirical evidence for the existence of demographic differentials in the majority of the face recognition algorithms we studied." They used four sets of photos containing 18 million images of 8.5 million people provided by various government agencies. Some findings:

 * for 1:1 matching, there were higher rates of false positives for Asian and African American faces relative to Caucasian faces, from factors of 10 to 100.
 * among US-developed algorithms, there were high rates of false positives for Asians, African Americans, and native groups
 * but among algorithms developed in Asian countries, there was no difference in false positives between Asian and Caucasian faces
 * the highest rates of false positives came among African American women
 * not all algorithms gave higher false positives across different races, and those that were the most equitable were also the most accurate.

**Thoughts**: that Asian-developed algorithms didn't exhibit higher false positives on Asian people, while US-developed algorithms did, is a great example of how these systems reflect the environments (and the biases thereof) in which they are created. A systemic study of this magnitude is great to see.

## [I asked my students to turn in their cell phones and write about living without them](https://www.technologyreview.com/s/614934/teenagers-without-cell-phones/)
**Author**: Ron Srigley

**How I found this**: from the MIT technology review

The title says it all: a professor offered extra credit to students willing to turn in their cell phones for nine days, then write about the experience. What the 12 students who participated found, Srigley writes, was generally positive and remarkably consistent. All of the students agreed that the ease of communication was a genuine benefit of their phones, though eight of the twelve also showed some relief from not having to be attentive to their phones. Unsurprisingly, many said that not having their phones made them more productive when writing essays or studying. One unfortunate recurring theme was people no longer being connected to their parents. Another was that cell phones improved one's safety in a world described as dangerous.

> My students’ experience of cell phones and the social-media platforms they support may not be exhaustive, or statistically representative. But it is clear that these gadgets made them feel less alive, less connected to other people and to the world, and less productive. They also made many tasks more difficult and encouraged students to act in ways they considered unworthy of themselves. In other words, phones didn’t help them. They harmed them.

**Thoughts**: kind of a strange article, but a really interesting experiment. If a professor offered this to me today, I would certainly take them up on it (yet why do I not choose to do this independently myself? hmm ...). There are lessons in this about how phones attempt to capture and maintain our attention, how apps are all optimizing for engagement, and how taking away my phone's control over my attention could restore the control back to me. I will be thinking about this more as we go into the new year.

