<!DOCTYPE html>
<html>

<head>
    <title>What I read this week - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="what_i_read_20191214">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week</h1>
<p>Similar to last week, here are summaries of blog posts, articles, or news that I read this week.</p>
<h2><a href="https://www.theatlantic.com/technology/archive/2018/03/largest-study-ever-fake-news-mit-twitter/555104/">The grim conclusions of the largest-ever study of fake news</a></h2>
<p><strong>Author:</strong> Robinson Meyer</p>
<p><strong>How I found this</strong>: ?</p>
<p>This is an article summarizing a research paper, <a href="http://science.sciencemag.org/cgi/doi/10.1126/science.aap9559">The spread of true and false news online</a>, by Vosoughi, Roy, and Aral in <em>Science</em> (that paper is on my list!). This study analyzed &quot;every major contested news study&quot; across Twitter, coming out to ~126K stories and 3M users over 10 years. The project was inspired by the Boston Marathon bombings in 2013, as the MIT researchers were confined to their own homes for days as the police searched for the suspects, hindered by a great deal of false information on Twitter (and <a href="https://www.theatlantic.com/national/archive/2013/04/reddit-find-boston-bombers-founder-interview/315987/">Reddit</a>). Following that, the researchers started studying how false information moved across Twitter. Its conclusions were grim: &quot;fake news&quot; reaches more people, penetrates deeper into social networks, and spreads much faster than accurate stories.</p>
<p>Fake news reached a larger audience (more total retweets) <em>and</em> propagated deeper into networks (longer average length of the retweet chains &quot;A retweeted B retweeted C ...&quot;). They identified two possible reasons: fake news seemed more &quot;novel,&quot; and that it evoked more emotion than the average tweet. This was true even in spite of the fact that users who shared accurate information had been on Twitter longer, were more likely to be verified, and had more followers—in other words, they had <em>every structural advantage possible</em>, and the truth still lost. Another key finding was that Twitter bots amplified true stories as much as false ones, so they're not to blame either.</p>
<blockquote>
<p>In short, social media seems to systematically amplify falsehood at the expense of the truth, and no one—neither experts nor politicians nor tech companies—knows how to reverse that trend. It is a dangerous moment for any system of government premised on a common public reality.</p>
</blockquote>
<p><strong>My commentary</strong>: the last bit (evoking more emotion) is one of the most worrisome problems in social media. The fact that platforms optimize for engagement, and engagement is often facilitated by outrage, speaks to the danger of social media. The amount of influence that they have over people's information consumption is even more worrisome. And it only seems to be getting worse, especially as our lawmakers seem increasingly technologically inept, the platforms continue to grow, and false information continues to provoke outrage.</p>
<h2><a href="https://venturebeat.com/2019/11/23/no-ai-is-not-for-social-good/">No, AI is not for social good</a></h2>
<p><strong>Author</strong>: Jared Moore</p>
<p><strong>How I found this</strong>: from the newsletter <a href="https://www.skynettoday.com/">Skynet Today</a></p>
<p>This article is essentially a critique of the idea of &quot;AI for social good,&quot; commonly tossed around as an application of advances in AI. One particular quote stands out:</p>
<blockquote>
<p>Likewise, the promise of “AI for the good” ignores the fact that problems like poverty, recidivism, and the distribution of resources are political ones; they’re often the results of institutional failure. Technologies, when not aimed at the root of problems, divert our attention. On top of that, do we really want to leave big tech to “solve” these social problems when it has shown it’s capable of creating substantial social problems of its own — I’m thinking here of Facebook with its Cambridge Analytica deal, for example.</p>
</blockquote>
<p>Put otherwise, technology cannot be used to solve complex social problems (see <a href="https://xkcd.com/1831/">xkcd</a>). Moreover, it's not unlikely that it's currently <em>creating</em> more complex social problems than it's solving. A great deal of &quot;AI for social good&quot; commentary is &quot;AI to [try to] solve [as of yet unsolvable] social issues,&quot; like policing, terrorism, healthcare, or content moderation. While it certainly offers great promise, taking a technology-first approach to these problems means we're all but certain to ignore, and possibly even amplify, institutional and systemic issues that create these problems in the first place.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>