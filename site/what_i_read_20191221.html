<!DOCTYPE html>
<html>

<head>
    <title>What I read this week - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="what_i_read_20191221">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week</h1>
<p>Summaries of articles and blog posts that I've read this week.</p>
<h2><a href="https://www.wired.com/story/sobering-message-future-ai-party">A sobering message about the future at AI's biggest party</a></h2>
<p><strong>Author:</strong> Tom Simonite</p>
<p><strong>How I found this</strong>: The <a href="https://www.skynettoday.com/">Skynet Today</a> newsletter</p>
<p><strong>Summary</strong>: This article talks about a recurring theme at NeurIPS 2019, which took place last week. &quot;Leaders in artificial intelligence warn that progress is slowing, big challenges remain, and simply throwing more computers at a problem isn't sustainable,&quot; reads the subheading. It's becoming more and more clear that this is true—advances in compute can, in my opinion, only take us so far, and researchers warned of the limitations of the current state of AI.</p>
<p><strong>Commentary</strong>: AI research is encountering some of the same roadblocks it always has—progress is slow and incremental, no one knows what or when the next breakthrough will be. But that's research!—that's always been the case. Now, though, the field has to combat increasing hype about AI, (somehow simultaneously) public skepticism (see, e.g., the <a href="https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html">biased algorithms</a> article from last week), and the looming threat of regulatory overreach. Facebook's head of AI Jerome Pesenti argued that AI research will soon hit the wall, but some might say it already has.</p>
<h2>[The AI community needs to take responsibility for its actions]</h2>
<p><strong>Author</strong>: Karen Hao</p>
<p><strong>How I found this</strong>: from Skynet Today</p>
<p>From the article, emphasis mine:</p>
<blockquote>
<p><strong>&quot;There’s no such thing as a neutral platform,&quot;</strong> the influential scientist [Celeste Kidd] and prominent #metoo figurehead told those gathered at the NeurIPS conference in Vancouver. &quot;The algorithms pushing content online have profound impacts on what we believe.&quot;</p>
</blockquote>
<p>As we consume more recommended or otherwise algorithmically curated content, there is no &quot;control group.&quot; YouTube can't suggest &quot;neutral&quot; recommendations short of showing us totally random suggestions—every algorithmic decision made impacts what we see in some way. Google can't show us &quot;neutral&quot; search results; they can only show us what they believe are &quot;best,&quot; &quot;most relevant,&quot; or (for the cynical) &quot;most likely to help them sell ads to us.&quot; As Kidd said, &quot;I don’t think there’s enough sensitivity in tech to how the decisions that you make behind the scenes about how to push content impact people’s lives.&quot;</p>
<p>This was awesome—I have to watch the talk itself soon.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>