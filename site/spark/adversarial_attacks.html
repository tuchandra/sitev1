<!DOCTYPE html>
<html>

<head>
    <title>Defending Deep Learning from Adversarial Attacks - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="adversarial_attacks">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>Defending Deep Learning from Adversarial Attacks</h1>
<p>Animesh Singh and Svetlana Levitan (IBM) - 4/25</p>
<hr />
<p>They work at the &quot;center for open source data and AI technologies&quot; @ IBM.</p>
<h2>Bias throughout the AI lifecycle</h2>
<p>There's an open source toolkit <a href="https://github.com/IBM/AIF360">AIF360</a> (&quot;AI Fairness 360&quot;) to monitor and remove bias in machine learning models. It includes 70+ fairness metrics and 10 mitigation algorithms to help remove those. One of those metrics is disparate impact (!!!) along with tons of others, and this actually seems incredible.</p>
<p>##Adversarial attacks</p>
<p>This field is getting a lot noisier (heh) and more research; adding white noise that is imperceptible to humans can blow up models and make their predictions vary wildly. Famous examples include the adversarial patch that makes everything appear to be a toaster; scarier ones include adding adversarial noise to self-driving cars.</p>
<p><strong>Poisoning attacks</strong> are performed at training, by inserting poisoned samples in training data. This lets a malicious agent use a backdoor later. <strong>Evasion attacks</strong> are performed at test time.</p>
<p>Threat models include:</p>
<ul>
<li><strong>black box vs. white box</strong>; what knowledge does the attacker have about the model? how do they access the model?</li>
<li><strong>plausible deniability</strong>: how important is it for the attacker to use adversarial samples that resemble original inputs?</li>
<li><strong>type I vs. II errors</strong>: bypass safeguards or increase false alarms?</li>
<li>one more that I missed</li>
</ul>
<p>Some evasion attacks push neural networks (or other models) out of their desired operating range — either finding a region of space that's sparse or pushing it out of the range that it was trained on. Key point is that DNNs don't learn to <em>recognize</em> a schoolbus, but rather to discriminate it from other objects in the training set.</p>
<h2>Defending adversarial attacks</h2>
<p><strong>Training solely on adversarial examples</strong>, which increases the capacity to maintain accuracy on clean data. Use a specific algorithm to intentionally introduce poisoned data as the model is trained.</p>
<p><strong>Preprocess data</strong> to remove adversarial noise, and input the cleaned samples into the classifier. But this can be defeated by an adaptive adversary (someone who knows how the cleaning is done).</p>
<p><strong>Robustness metrics</strong>, e.g., CLEVER, which evaluate how robust neural networks are. Paper: &quot;<a href="https://openreview.net/pdf?id=BkUHlMZ0b">Evaluating the Robustness of Neural Networks: An Extreme Value Theory</a>.&quot;</p>
<p><strong>Poisoning detection</strong>, where you look at clustering of the training data and see if it's been poisoned.</p>
<p>Many, many others …</p>
<p>All of this to say that IBM built the <a href="https://github.com/IBM/adversarial-robustness-toolbox">Adversarial Robustness Toolbox</a>, which is open-source and is a library for adversarial machine learning. It includes evasion attacks, evasion defenses, poisioning and evasion detection, and robustness metrics.</p>
<h2>Demos</h2>
<p>These were all super slick. This <a href="https://art-demo.mybluemix.net/">demo</a> lets you interactively add imperceptible noise to images, and wow this is so cool. Then they showed some Jupyter notebooks with code samples from their toolbox.</p>
<p>Another paper: <a href="https://arxiv.org/pdf/1706.06083.pdf">Towards Deep Learning Models Resistant to Adversarial Attacks</a>.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>