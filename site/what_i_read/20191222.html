<!DOCTYPE html>
<html>

<head>
    <title>What I read this week (December 22 - 28) - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="20191222">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week (December 22 - 28)</h1>
<h2><a href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">This is how AI bias really happens--and why it's so hard to fix</a></h2>
<p><strong>Author</strong>: Karen Hao</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p><strong>Summary</strong>: it's easy to say that bias in AI comes from bias in training data, but that's an oversimplification; why does the training data contain biased? Why can't we just find unbiased training data, if that's the case? The reality is, of course, more complex, and Hao outlines three places where bias creeps in:</p>
<ul>
<li><em>framing the problem</em>, where you define what your goal is, how you compute it, and how it relates to business objectives.</li>
<li><em>collecting the data</em>, where the data you collect is unrepresentative of reality (images of only white people), or it reflects existing prejudices (training on historical decisions about who to hire).</li>
<li><em>preparing the data</em>, where you choose features that may lead to a more biased model.</li>
</ul>
<p>Why, then, is it so hard to fix? Some reasons include:</p>
<ul>
<li><em>unknown unknowns</em>, where you don't know when a choice you make introduces bias, and it isn't apparent until much later</li>
<li>we have <em>imperfect processes</em> that aren't designed with the goal of reducing bias, like how you evaluate a model on data that's like the training dataset instead of in the real world.</li>
<li><em>lack of social context</em>, like how &quot;fair&quot; means different things in different communities, how generalization in a CS setting is different than generalization in a social setting, etc.</li>
<li><em>unclear definitions of fairness</em>, which doesn't have a clear definition in the real world, leave alone one that we can apply to statistical models.</li>
</ul>
<p><strong>Thoughts</strong>: this is good. I'm definitely guilty of using the term &quot;AI bias&quot; or &quot;biased models&quot; imprecisely, and pieces like this help me to think more about what that actually means and where those biases come from.</p>
<h2><a href="https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software">NIST study evaluates effects of age, race, sex on face recognition software</a></h2>
<p><strong>Author</strong>: NIST</p>
<p><strong>How I found this</strong>: from the newsletter <a href="https://www.the-algorithm.net/newsletter">The Algorithm</a></p>
<p><strong>Summary</strong>: NIST studied 189 algorithms among 99 developers, and &quot;found empirical evidence for the existence of demographic differentials in the majority of the face recognition algorithms we studied.&quot; They used four sets of photos containing 18 million images of 8.5 million people provided by various government agencies. Some findings:</p>
<ul>
<li>for 1:1 matching, there were higher rates of false positives for Asian and African American faces relative to Caucasian faces, from factors of 10 to 100.</li>
<li>among US-developed algorithms, there were high rates of false positives for Asians, African Americans, and native groups</li>
<li>but among algorithms developed in Asian countries, there was no difference in false positives between Asian and Caucasian faces</li>
<li>the highest rates of false positives came among African American women</li>
<li>not all algorithms gave higher false positives across different races, and those that were the most equitable were also the most accurate.</li>
</ul>
<p><strong>Thoughts</strong>: that Asian-developed algorithms didn't exhibit higher false positives on Asian people, while US-developed algorithms did, is a great example of how these systems reflect the environments (and the biases thereof) in which they are created. A systemic study of this magnitude is great to see.</p>
<h2><a href="https://www.technologyreview.com/s/614934/teenagers-without-cell-phones/">I asked my students to turn in their cell phones and write about living without them</a></h2>
<p><strong>Author</strong>: Ron Srigley</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p>The title says it all: a professor offered extra credit to students willing to turn in their cell phones for nine days, then write about the experience. What the 12 students who participated found, Srigley writes, was generally positive and remarkably consistent. All of the students agreed that the ease of communication was a genuine benefit of their phones, though eight of the twelve also showed some relief from not having to be attentive to their phones. Unsurprisingly, many said that not having their phones made them more productive when writing essays or studying. One unfortunate recurring theme was people no longer being connected to their parents. Another was that cell phones improved one's safety in a world described as dangerous.</p>
<blockquote>
<p>My students’ experience of cell phones and the social-media platforms they support may not be exhaustive, or statistically representative. But it is clear that these gadgets made them feel less alive, less connected to other people and to the world, and less productive. They also made many tasks more difficult and encouraged students to act in ways they considered unworthy of themselves. In other words, phones didn’t help them. They harmed them.</p>
</blockquote>
<p><strong>Thoughts</strong>: kind of a strange article, but a really interesting experiment. If a professor offered this to me today, I would certainly take them up on it (yet why do I not choose to do this independently myself? hmm ...). There are lessons in this about how phones attempt to capture and maintain our attention, how apps are all optimizing for engagement, and how taking away my phone's control over my attention could restore the control back to me. I will be thinking about this more as we go into the new year.</p>
<h2><a href="https://alexnixon.github.io/2019/12/10/writing.html">It’s time to start writing</a></h2>
<p><strong>How I found this</strong>: from /r/programming</p>
<p><strong>Summary</strong>: this is a blog post that someone wrote reflecting on the Bezos memo telling Amazon folks to start writing memos before meetings. The author argues “it’s creation which is important, not consumption;” that the act of writing forces you to organize your thoughts in a coherent way, and this is the true value in writing often.</p>
<p><strong>Thoughts</strong>: this is very true, and it’s one of the reasons I started this blog. I don’t think anyone reads it, but I don’t really care. I get a tremendous amount of value by <a href="https://tusharc.dev/site/posts/why_papers.html">writing paper summaries</a>, summarizing and reflecting on talks, and even discussing other short articles that I read (like this one). Writing is my way of engaging with the material, and it helps me to develop my thoughts on different subjects and fields.</p>
<blockquote>
<p>Writing is thinking. To write well is to think clearly. That's why it's so hard.</p>
</blockquote>
<h2><a href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/yoshua-bengio-revered-architect-of-ai-has-some-ideas-about-what-to-build-next">Interview with Yoshua Bengio</a></h2>
<p><strong>How I found this</strong>: from my friend sending it to me</p>
<p><strong>Summary</strong>: this is an interview with Turing award winner Yoshua Bengio about the state of deep learning and ideas about what to build next. One theme is that it’s not currently anywhere near human cognition, but it’s still a very powerful tool. Another is that human cognition continues to be an inspiration for where to go next with building AI systems. Another still is that a lot of the work in the field isn’t ready for real applications yet, and that we’re still in the basic research phase of a lot of it.</p>
<p><strong>Thoughts</strong>: media portrayals of deep learning are, in my experience, either “this is achieving superhuman performance” or “this field creates biased systems and fails spectacularly on certain cases.” The truth, of course, is somewhere in the middle. It’s a powerful tool, but not suitable for everything (see the <a href="https://tusharc.dev/site/posts/talk_aguera_neurips.html">interview with Aguera</a> where he talks about how a surprisingly large number of real problems can’t be defined in terms of loss functions). This was good.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>