<!DOCTYPE html>
<html>

<head>
    <title>What I read this week (December 22 - 28) - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="20191222">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week (December 22 - 28)</h1>
<h2><a href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">This is how AI bias really happens--and why it's so hard to fix</a></h2>
<p><strong>Author</strong>: Karen Hao</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p><strong>Summary</strong>: it's easy to say that bias in AI comes from bias in training data, but that's an oversimplification; why does the training data contain biased? Why can't we just find unbiased training data, if that's the case? The reality is, of course, more complex, and Hao outlines three places where bias creeps in:</p>
<ul>
<li><em>framing the problem</em>, where you define what your goal is, how you compute it, and how it relates to business objectives.</li>
<li><em>collecting the data</em>, where the data you collect is unrepresentative of reality (images of only white people), or it reflects existing prejudices (training on historical decisions about who to hire).</li>
<li><em>preparing the data</em>, where you choose features that may lead to a more biased model.</li>
</ul>
<p>Why, then, is it so hard to fix? Some reasons include:</p>
<ul>
<li><em>unknown unknowns</em>, where you don't know when a choice you make introduces bias, and it isn't apparent until much later</li>
<li>we have <em>imperfect processes</em> that aren't designed with the goal of reducing bias, like how you evaluate a model on data that's like the training dataset instead of in the real world.</li>
<li><em>lack of social context</em>, like how &quot;fair&quot; means different things in different communities, how generalization in a CS setting is different than generalization in a social setting, etc.</li>
<li><em>unclear definitions of fairness</em>, which doesn't have a clear definition in the real world, leave alone one that we can apply to statistical models.</li>
</ul>
<p><strong>Thoughts</strong>: this is good. I'm definitely guilty of using the term &quot;AI bias&quot; or &quot;biased models&quot; imprecisely, and pieces like this help me to think more about what that actually means and where those biases come from.</p>
<h2><a href="https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software">NIST study evaluates effects of age, race, sex on face recognition software</a></h2>
<p><strong>Author</strong>: NIST</p>
<p><strong>How I found this</strong>: from the newsletter <a href="https://www.the-algorithm.net/newsletter">The Algorithm</a></p>
<p><strong>Summary</strong>: NIST studied 189 algorithms among 99 developers, and &quot;found empirical evidence for the existence of demographic differentials in the majority of the face recognition algorithms we studied.&quot; They used four sets of photos containing 18 million images of 8.5 million people provided by various government agencies. Some findings:</p>
<ul>
<li>for 1:1 matching, there were higher rates of false positives for Asian and African American faces relative to Caucasian faces, from factors of 10 to 100.</li>
<li>among US-developed algorithms, there were high rates of false positives for Asians, African Americans, and native groups</li>
<li>but among algorithms developed in Asian countries, there was no difference in false positives between Asian and Caucasian faces</li>
<li>the highest rates of false positives came among African American women</li>
<li>not all algorithms gave higher false positives across different races, and those that were the most equitable were also the most accurate.</li>
</ul>
<p><strong>Thoughts</strong>: that Asian-developed algorithms didn't exhibit higher false positives on Asian people, while US-developed algorithms did, is a great example of how these systems reflect the environments (and the biases thereof) in which they are created. A systemic study of this magnitude is great to see.</p>
<h2><a href="https://www.technologyreview.com/s/614934/teenagers-without-cell-phones/">I asked my students to turn in their cell phones and write about living without them</a></h2>
<p><strong>Author</strong>: Ron Srigley</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p>The title says it all: a professor offered extra credit to students willing to turn in their cell phones for nine days, then write about the experience. What the 12 students who participated found, Srigley writes, was generally positive and remarkably consistent. All of the students agreed that the ease of communication was a genuine benefit of their phones, though eight of the twelve also showed some relief from not having to be attentive to their phones. Unsurprisingly, many said that not having their phones made them more productive when writing essays or studying. One unfortunate recurring theme was people no longer being connected to their parents. Another was that cell phones improved one's safety in a world described as dangerous.</p>
<blockquote>
<p>My students’ experience of cell phones and the social-media platforms they support may not be exhaustive, or statistically representative. But it is clear that these gadgets made them feel less alive, less connected to other people and to the world, and less productive. They also made many tasks more difficult and encouraged students to act in ways they considered unworthy of themselves. In other words, phones didn’t help them. They harmed them.</p>
</blockquote>
<p><strong>Thoughts</strong>: kind of a strange article, but a really interesting experiment. If a professor offered this to me today, I would certainly take them up on it (yet why do I not choose to do this independently myself? hmm ...). There are lessons in this about how phones attempt to capture and maintain our attention, how apps are all optimizing for engagement, and how taking away my phone's control over my attention could restore the control back to me. I will be thinking about this more as we go into the new year.</p>
<h2><a href="https://alexnixon.github.io/2019/12/10/writing.html">It’s time to start writing</a></h2>
<p><strong>How I found this</strong>: from /r/programming</p>
<p><strong>Summary</strong>: this is a blog post that someone wrote reflecting on the Bezos memo telling Amazon folks to start writing memos before meetings. The author argues “it’s creation which is important, not consumption;” that the act of writing forces you to organize your thoughts in a coherent way, and this is the true value in writing often.</p>
<p><strong>Thoughts</strong>: this is very true, and it’s one of the reasons I started this blog. I don’t think anyone reads it, but I don’t really care. I get a tremendous amount of value by <a href="https://tusharc.dev/site/posts/why_papers.html">writing paper summaries</a>, summarizing and reflecting on talks, and even discussing other short articles that I read (like this one). Writing is my way of engaging with the material, and it helps me to develop my thoughts on different subjects and fields.</p>
<blockquote>
<p>Writing is thinking. To write well is to think clearly. That's why it's so hard.</p>
</blockquote>
<h2><a href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/yoshua-bengio-revered-architect-of-ai-has-some-ideas-about-what-to-build-next">Interview with Yoshua Bengio</a></h2>
<p><strong>How I found this</strong>: from my friend sending it to me</p>
<p><strong>Summary</strong>: this is an interview with Turing award winner Yoshua Bengio about the state of deep learning and ideas about what to build next. One theme is that it’s not currently anywhere near human cognition, but it’s still a very powerful tool. Another is that human cognition continues to be an inspiration for where to go next with building AI systems. Another still is that a lot of the work in the field isn’t ready for real applications yet, and that we’re still in the basic research phase of a lot of it.</p>
<p><strong>Thoughts</strong>: media portrayals of deep learning are, in my experience, either “this is achieving superhuman performance” or “this field creates biased systems and fails spectacularly on certain cases.” The truth, of course, is somewhere in the middle. It’s a powerful tool, but not suitable for everything (see the <a href="https://tusharc.dev/site/posts/talk_aguera_neurips.html">interview with Aguera</a> where he talks about how a surprisingly large number of real problems can’t be defined in terms of loss functions). This was good.</p>
<h2><a href="https://www.brookings.edu/research/public-opinion-lessons-for-ai-regulation/">Public opinion lessons for AI regulation</a></h2>
<p><strong>How I found this</strong>: from Skynet Today</p>
<p><strong>Summary</strong>: this is a brief from the Brookings Institute that discusses public opinion about different types of AI regulation: law enforcement's use of facial recognition, algorithms used by social media platforms, and lethal autonomous weapons.</p>
<ul>
<li><strong>Facial recognition</strong>: a majority of adults (59%) find the use of facial recognition by law enforcement acceptable, though this number is lower among people ages 18 - 29 (42%), Black and Hispanic people (47% and 55% respectively), and Democrats (55%, vs 67% for Republicans). Brookings just summarized a <a href="https://www.pewresearch.org/internet/2019/09/05/more-than-half-of-u-s-adults-trust-law-enforcement-to-use-facial-recognition-responsibly/">Pew Research report</a> here, though, which is kind of unfortunate.</li>
<li><strong>Social media</strong>: a slight majority of adults (51%) think that &quot;tech companies&quot; should be regulated more than they are today. But they also somehow have <em>more</em> trust in tech companies than the US government to manage the development of AI. Republicans are far more likely than Democrats to think that social media platforms censor political viewpoints (54% vs. 20%), and less likely to think that tech companies support the views of liberals and conservative equally (28% vs. 53%).</li>
<li><strong>Lethal autonomous weapons</strong>: a slight majority of adults (51%) oppose the use of fully autonomous weapons systems. There are fewer stats here, but critics of this tech have had gathered support among powerful people, like Elon Musk and the founders of DeepMind. Notably, Google cancelled the Pentagon Project to improve the precision of drone strikes.</li>
</ul>
<p><strong>Thoughts</strong>: computer scientists and AI / ML researchers and practitioners are certainly aware that the term &quot;AI&quot; means different things in different scenarios, but it's refreshing to see a report like this that breaks down three <em>very</em> different applications of things that are all technically AI. The actual statistics quoted are unsurprising; facial recognition is less likely to be supported by young people, liberals, and minorities, surprise! Republicans believe that social media companies censor their views, which I don't personally believe is true but I understand why they would be more likely to think that.</p>
<p>Though it was only an offhand comment in the article, the question of &quot;what is a tech company&quot; is important to consider. That Americans want to see more regulation of tech companies doesn't say anything about how that regulation should occur; the business models of Facebook, Google, and Amazon are all very different. I'm heavily influenced by Ben Thompson's <a href="https://stratechery.com">Stratechery</a> here, where he wrote an <a href="https://stratechery.com/2019/where-warrens-wrong/">article</a> on Elizabeth Warren's ideas for tech regulation:</p>
<blockquote>
<p>That leads to a broader point: <strong>“tech” is not simply another category, like railroads or telecom. Tech is a means, not an end,</strong> but Senator Warren’s approach presumes the latter. That is why she proposes the same set of rules for the sale of toasters and the sale of apps, and everything in between. The truth is that Amazon is a retailer; Apple a combination of hardware maker and platform makers. Google is a search and advertising company, and Facebook a publishing and advertising company. They all have different value chains and different ways of impacting competition, both fairly and unfairly, and to fail to appreciate just how different they are is a great way to make bad laws that not only fail to fix problems but also create entirely new ones.</p>
</blockquote>
<p>This is why &quot;regulating tech&quot; is, to me, a meaningless idea. One can't hope to regulate Google, Facebook, Apple, and Amazon the same way, because they all have different business models and different ways of affecting their competition. The story is the same with AI: we can't just &quot;regulate AI,&quot; because that encompasses everything from linear regressions to deepfake videos to facial recognition. What we <em>can</em> do is create guidelines for responsible use of different technologies,
develop interpretable algorithms, understand how AI reinforces and avoids existing biases, study who it gives power to and who it takes power away from, and more.</p>
<h2>Other articles</h2>
<p><a href="https://mostlycloudy.substack.com/p/why-machine-learning-cant-save-the">Why machine learning can't save the NFL</a>, from <a href="https://dataelixir.com/">Data Elixir</a>, talks about how the NFL has signed a huge contract with AWS to try to &quot;reinvent itself&quot; for the health of its players. But it's unclear what the NFL thinks it can do here by putting all of its data in the cloud. Anyone remotely familiar with football will tell you that it's dangerous, and while major injuries do often come off single plays, those are (1) impossible to predict (2) not where the lasting brain damage comes from, which just comes from repeated hard-hitting plays. The sport is dangerous, and the cloud and machine learning can't stop the injuries that are bound to come from large, strong people hitting each other with their bodies week after week.</p>
<p><a href="https://hackernoon.com/algorithms-arent-racist-your-skin-is-just-too-dark-4ed31a7304b8">Algorithms aren't racist; your skin is just too dark</a> is a provocatively-titled HackerNoon article about (surprise!) AI bias. The author asks the question of whether or not ordinary cameras are objective; they perform worse by default on people with darker skin (and other dark subjects, like my black cats!). The defaults, too, are not neutral; those are chosen based on the preferences of those who develop the technology. The article doesn't really pose any questions I haven't heard before, but the &quot;defaults aren't neutral&quot; ties in nicely with <a href="https://tusharc.dev/site/posts/talk_kidd_neurips.html">Celeste Kidd's talk</a> that I wrote about earlier.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>