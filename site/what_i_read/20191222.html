<!DOCTYPE html>
<html>

<head>
    <title>What I read this week (December 22 - 28) - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="20191222">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week (December 22 - 28)</h1>
<p>Summaries of articles and blog posts that I've read this week.</p>
<h2><a href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">This is how AI bias really happens--and why it's so hard to fix</a></h2>
<p><strong>Author</strong>: Karen Hao</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p><strong>Summary</strong>: it's easy to say that bias in AI comes from bias in training data, but that's an oversimplification; why does the training data contain biased? Why can't we just find unbiased training data, if that's the case? The reality is, of course, more complex, and Hao outlines three places where bias creeps in:</p>
<ul>
<li><em>framing the problem</em>, where you define what your goal is, how you compute it, and how it relates to business objectives.</li>
<li><em>collecting the data</em>, where the data you collect is unrepresentative of reality (images of only white people), or it reflects existing prejudices (training on historical decisions about who to hire).</li>
<li><em>preparing the data</em>, where you choose features that may lead to a more biased model.</li>
</ul>
<p>Why, then, is it so hard to fix? Some reasons include:</p>
<ul>
<li><em>unknown unknowns</em>, where you don't know when a choice you make introduces bias, and it isn't apparent until much later</li>
<li>we have <em>imperfect processes</em> that aren't designed with the goal of reducing bias, like how you evaluate a model on data that's like the training dataset instead of in the real world.</li>
<li><em>lack of social context</em>, like how &quot;fair&quot; means different things in different communities, how generalization in a CS setting is different than generalization in a social setting, etc.</li>
<li><em>unclear definitions of fairness</em>, which doesn't have a clear definition in the real world, leave alone one that we can apply to statistical models.</li>
</ul>
<p><strong>Thoughts</strong>: this is good. I'm definitely guilty of using the term &quot;AI bias&quot; or &quot;biased models&quot; imprecisely, and pieces like this help me to think more about what that actually means and where those biases come from.</p>
<h2><a href="https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software">NIST study evaluates effects of age, race, sex on face recognition software</a></h2>
<p><strong>Author</strong>: NIST</p>
<p><strong>How I found this</strong>: from the newsletter <a href="https://www.the-algorithm.net/newsletter">The Algorithm</a></p>
<p><strong>Summary</strong>: NIST studied 189 algorithms among 99 developers, and &quot;found empirical evidence for the existence of demographic differentials in the majority of the face recognition algorithms we studied.&quot; They used four sets of photos containing 18 million images of 8.5 million people provided by various government agencies. Some findings:</p>
<ul>
<li>for 1:1 matching, there were higher rates of false positives for Asian and African American faces relative to Caucasian faces, from factors of 10 to 100.</li>
<li>among US-developed algorithms, there were high rates of false positives for Asians, African Americans, and native groups</li>
<li>but among algorithms developed in Asian countries, there was no difference in false positives between Asian and Caucasian faces</li>
<li>the highest rates of false positives came among African American women</li>
<li>not all algorithms gave higher false positives across different races, and those that were the most equitable were also the most accurate.</li>
</ul>
<p><strong>Thoughts</strong>: that Asian-developed algorithms didn't exhibit higher false positives on Asian people, while US-developed algorithms did, is a great example of how these systems reflect the environments (and the biases thereof) in which they are created. A systemic study of this magnitude is great to see.</p>
<h2></h2>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>