<!DOCTYPE html>
<html>

<head>
    <title>What I read this week (December 22 - 28) - Tushar Chandra</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" type="text/css" href="/site/style.css">
</head>

<body id="20191222">

    <nav>
        <section>
            <span class="home">
                <a href="/site/">Home</a>
            </span>
            <span class="links">
                //
                <a href="/site/contact.html">Contact</a>
                //
                <a href="/site/resume.html">Resume</a>
                //
                <a href="/site/projects.html">Projects</a>
                //
                <a href="/site/reading_list.html">Reading List</a>
            </span>
        </section>
    </nav>

    <main>
        <h1>What I read this week (December 22 - 28)</h1>
<p>Summaries of articles and blog posts that I've read this week.</p>
<p>...</p>
<h2><a href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/">This is how AI bias really happens--and why it's so hard to fix</a></h2>
<p><strong>Authoor</strong>: Karen Hao</p>
<p><strong>How I found this</strong>: from the MIT technology review</p>
<p><strong>Summary</strong>: it's easy to say that bias in AI comes from bias in training data, but that's an oversimplification; why does the training data contain biased? Why can't we just find unbiased training data, if that's the case? The reality is, of course, more complex, and Hao outlines three places where bias creeps in:</p>
<ul>
<li><em>framing the problem</em>, where you define what your goal is, how you compute it, and how it relates to business objectives.</li>
<li><em>collecting the data</em>, where the data you collect is unrepresentative of reality (images of only white people), or it reflects existing prejudices (training on historical decisions about who to hire).</li>
<li><em>preparing the data</em>, where you choose features that may lead to a more biased model.</li>
</ul>
<p>Why, then, is it so hard to fix? Some reasons include:</p>
<ul>
<li><em>unknown unknowns</em>, where you don't know when a choice you make introduces bias, and it isn't apparent until much later</li>
<li>we have <em>imperfect processes</em> that aren't designed with the goal of reducing bias, like how you evaluate a model on data that's like the training dataset instead of in the real world.</li>
<li><em>lack of social context</em>, like how &quot;fair&quot; means different things in different communities, how generalization in a CS setting is different than generalization in a social setting, etc.</li>
<li><em>unclear definitions of fairness</em>, which doesn't have a clear definition in the real world, leave alone one that we can apply to statistical models.</li>
</ul>
<p><strong>Thoughts</strong>: this is good. I'm definitely guilty of using the term &quot;AI bias&quot; or &quot;biased models&quot; imprecisely, and pieces like this help me to think more about what that actually means and where those biases come from.</p>

    </main>

    <footer>
        <section>
            <p>&copy; 2019 Tushar Chandra
                (<a href="https://github.com/tuchandra">GitHub</a> // <a href="https://tusharc.dev/">Website</a>)
            </p>
        </section>
    </footer>

</body>

</html>